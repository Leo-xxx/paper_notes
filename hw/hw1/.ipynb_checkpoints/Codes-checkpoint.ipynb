{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./hw1_data/LFW_DATA.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#print data.keys()\n",
    "#[u'database_name', u'query_identity', u'database_identity', u'query_name', u'database_feature', u'query_feature']\n",
    "src = data['database_feature']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tperson\timagenum\tMale\tAsian\tWhite\tBlack\tBaby\tChild\tYouth\tMiddle Aged\tSenior\tBlack Hair\tBlond Hair\tBrown Hair\tBald\tNo Eyewear\tEyeglasses\tSunglasses\tMustache\tSmiling\tFrowning\tChubby\tBlurry\tHarsh Lighting\tFlash\tSoft Lighting\tOutdoor\tCurly Hair\tWavy Hair\tStraight Hair\tReceding Hairline\tBangs\tSideburns\tFully Visible Forehead\tPartially Visible Forehead\tObstructed Forehead\tBushy Eyebrows\tArched Eyebrows\tNarrow Eyes\tEyes Open\tBig Nose\tPointy Nose\tBig Lips\tMouth Closed\tMouth Slightly Open\tMouth Wide Open\tTeeth Not Visible\tNo Beard\tGoatee\tRound Jaw\tDouble Chin\tWearing Hat\tOval Face\tSquare Face\tRound Face\tColor Photo\tPosed Photo\tAttractive Man\tAttractive Woman\tIndian\tGray Hair\tBags Under Eyes\tHeavy Makeup\tRosy Cheeks\tShiny Skin\tPale Skin\t5 o' Clock Shadow\tStrong Nose-Mouth Lines\tWearing Lipstick\tFlushed Face\tHigh Cheekbones\tBrown Eyes\tWearing Earrings\tWearing Necktie\tWearing Necklace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./hw1_data/lfw_attributes.txt') as f:\n",
    "    ls = f.readlines()\n",
    "print(ls[1])\n",
    "at = ls[2:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[George W Bush][4] index not in dict. [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530]\n",
      "[Jacques Chirac][6] index not in dict. [1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]\n",
      "[Jacques Chirac][10] index not in dict. [1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]\n",
      "miss value  3\n"
     ]
    }
   ],
   "source": [
    "# prepare dict for attributes with name\n",
    "with open('./hw1_data/lfw_attributes.txt') as f:\n",
    "    ls = f.readlines()\n",
    "at = ls[2:]\n",
    "attribute_dict = dict()\n",
    "for l in at:\n",
    "    atts = l.strip().split('\\t')\n",
    "#     print atts\n",
    "    name,idx,_,_,white = atts[0:5]\n",
    "    if not attribute_dict.has_key(name):\n",
    "        attribute_dict[name] = dict()\n",
    "    attribute_dict[name][int(idx)] = float(white)\n",
    "\n",
    "# missing attributes\n",
    "file_names = data['database_identity'].ravel()\n",
    "miss = 0\n",
    "file_names = data['query_identity'].ravel()\n",
    "for f in file_names:\n",
    "    s = f[0].split('\\\\')[-1].split('.jpg')[0]\n",
    "    idx = int(s.split('_')[-1])\n",
    "    buf = s.replace('_',' ').split(' ')[:-1]\n",
    "#     \n",
    "    name = ''\n",
    "    for b in buf:\n",
    "        name += b\n",
    "        name += ' '\n",
    "    name = name.strip()\n",
    "#         \n",
    "    if not attribute_dict.has_key(name):\n",
    "        print '[%s] not in dict.' % name\n",
    "        miss += 1\n",
    "    else:\n",
    "        if not attribute_dict[name].has_key(idx):\n",
    "            print '[%s][%d] index not in dict.' % (name, idx), attribute_dict[name].keys()\n",
    "            miss+=1\n",
    "#     print name, idx\n",
    "print 'miss value ',miss\n",
    "# print ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 4720)\n",
      "(59, 80, 13113)\n"
     ]
    }
   ],
   "source": [
    "import spams\n",
    "import numpy as np\n",
    "print(src.shape)\n",
    "num = src.shape[0]\n",
    "X = src.reshape([num, 80,59])\n",
    "\n",
    "X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "\n",
    "lambda1 = [10**i for i in range(-6,-3,1)]\n",
    "K = range(100,3200,400)\n",
    "param = { 'K' : 400, # learns a dictionary with 100 elements\n",
    "          'lambda1' : 10**-2, 'numThreads' : -1, 'mode':2,\n",
    "          'iter' : 1000}\n",
    "\n",
    "    \n",
    "print(X.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (59, 120)\n",
      "d (59, 60)\n",
      "w (60, 120)\n",
      "x (59, 13113)\n",
      "d (59, 60)\n",
      "w (60, 13113)\n",
      "(60,)\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False  True  True\n",
      " False False False  True False False False False False False False False\n",
      " False  True  True  True False False  True False  True False  True  True]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import spams\n",
    "import numpy as np\n",
    "import scipy\n",
    "def train_SC(database, param, out_file='./dict.npy'):\n",
    "\n",
    "\n",
    "    num = database.shape[0]\n",
    "    X = database.reshape([num, 80,59])\n",
    "    X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "\n",
    "    D_list = []\n",
    "    pathces_num = X.shape[1]\n",
    "#     for i in range(pathces_num):\n",
    "    for i in range(1):\n",
    "        x = np.asfortranarray(X[:,i,:])\n",
    "        D = spams.trainDL(x,**param)\n",
    "        D_list.append(D)\n",
    "#     with open(out_file,'wb') as f:\n",
    "#         np.save(f,D_list)\n",
    "#     print('Done')\n",
    "#     with open(out_file,'rb') as f:\n",
    "#         data = np.load(f)\n",
    "#     print(data.shape)\n",
    "    return D_list\n",
    "def sparse_coding(q_features, db_features, param):\n",
    "    qs = q_features.reshape([len(q_features), 80,59])\n",
    "    qs = np.swapaxes(qs,axis1=0,axis2=2)\n",
    "    X = db_features.reshape([len(db_features), 80,59])\n",
    "    X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "    \n",
    "    D_list = train_SC(db_features, param)\n",
    "    q_codeword = get_codeword(D_list, qs)\n",
    "    db_codeword = get_codeword(D_list, X)\n",
    "    \n",
    "    return q_codeword, db_codeword\n",
    "def get_codeword(D_list, X,lambda1=0.01):\n",
    "    n = X.shape[-1]\n",
    "    W = np.ones([60,n],dtype=float)\n",
    "    W[:30] = float('inf')\n",
    "    param = { 'lambda1' : lambda1,'numThreads' : -1, 'mode':2, 'W':np.asfortranarray(W)}\n",
    "    buf = []\n",
    "    pathces_num = len(D_list)\n",
    "    for i in range(pathces_num):\n",
    "        q = np.asfortranarray(X[:,i,:])\n",
    "        D = D_list[i]\n",
    "        print 'x', q.shape\n",
    "        print 'd', D.shape\n",
    "        print 'w', W.shape\n",
    "        \n",
    "        a = spams.lassoWeighted(q,D=D,**param)\n",
    "        buf.append(a)\n",
    "    buf = scipy.sparse.vstack(buf)\n",
    "    codeword = np.zeros(buf.shape, dtype=np.bool)\n",
    "    codeword[buf.nonzero()] = True\n",
    "    \n",
    "    \n",
    "    codeword = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "    \n",
    "    return codeword\n",
    "database = data['database_feature']\n",
    "queries = data['query_feature']\n",
    "labels = data['database_name'].ravel()\n",
    "query_labels = data['query_name'].ravel()\n",
    "\n",
    "#part 1: l2_distance and calculate_map\n",
    "# mAP = calculate_map(database, labels, queries, query_labels, l2_distance)\n",
    "# print(mAP)\n",
    "\n",
    "#part 2: sparse_coding\n",
    "pos = np.asfortranarray([3]*200)\n",
    "param = { 'K' : 60,\n",
    "                  'lambda1' : 0.01, 'numThreads' : -1, 'mode':2,\n",
    "                  'iter' : 3}\n",
    "        \n",
    "q_sparse, db_sparse = sparse_coding(queries, database, param) #build sparse dict and lookup using spams library\n",
    "# sparse_map = calculate_map(db_sparse, labels, q_sparse, query_labels, similarity_sparse_coding)# results_sparse = distance(q_sparse, db_sparse) #you can use l2_distance in part1 or try any distance metric, like cos, l1 \n",
    "\n",
    "# sparse_coding(queries, database, param)\n",
    "q = q_sparse[0,:]\n",
    "print q.shape\n",
    "print q\n",
    "print 'Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 80, 13113)\n",
      "(32000, 120)\n",
      "(32000, 13113)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import spams\n",
    "# print D\n",
    "pathces_num = 80\n",
    "test = data['query_feature']\n",
    "num = test.shape[0]\n",
    "qs = test.reshape([num, 80,59])\n",
    "qs = np.swapaxes(qs,axis1=0,axis2=2)\n",
    "# \n",
    "with open('./dict2.npy','rb') as f:\n",
    "    D_list = np.load(f)\n",
    "    D_list = [np.asfortranarray(D) for D in D_list]\n",
    "def get_codeword(D_list, X,lambda1=0.01):\n",
    "    \n",
    "    param = { 'lambda1' : lambda1, 'lambda2' : 0,'numThreads' : -1, 'mode':2}\n",
    "    buf = []\n",
    "    for i in range(pathces_num):\n",
    "        q = np.asfortranarray(X[:,i,:])\n",
    "        D = D_list[i]\n",
    "        a = spams.lasso(q,D=D,return_reg_path = False,**param)\n",
    "        buf.append(a)\n",
    "    buf = scipy.sparse.vstack(buf)\n",
    "    codeword = np.zeros(buf.shape, dtype=np.bool)\n",
    "    codeword[buf.nonzero()] = True\n",
    "    return codeword\n",
    "\n",
    "#\n",
    "num = src.shape[0]\n",
    "X = src.reshape([num, 80,59])\n",
    "\n",
    "X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "print(X.shape)\n",
    "# \n",
    "q_codeword = get_codeword(D_list, qs)\n",
    "codeword = get_codeword(D_list, X)\n",
    "print(q_codeword.shape)\n",
    "print(codeword.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 32000) (120, 32000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mAp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e15e09fd5eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcalculate_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_sparse_coding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# mAP :  0.13721717999431532\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# mAP : 0.1581216311119271 with 800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/paper_notes/hw/hw1/calculate_map.py\u001b[0m in \u001b[0;36mcalculate_map\u001b[0;34m(database, labels, queries, query_labels, similarity_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mAPs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         print i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mAp' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# 注意要計算and而不是xor(hamming distance)\n",
    "def similarity_sparse_coding(codeword, query):\n",
    "    q = query.ravel()\n",
    "    \n",
    "    distances = []\n",
    "    for x in codeword:\n",
    "#         distances.append(scipy.spatial.distance.hamming(x,query))\n",
    "        distances.append(np.sum(np.logical_and(x,q)))\n",
    "    return reversed(np.argsort(distances))\n",
    "\n",
    "\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "labels = data['database_name'].ravel()\n",
    "\n",
    "queries = np.swapaxes(q_codeword, axis1=0, axis2=1)\n",
    "query_labels = data['query_name'].ravel()\n",
    "print(database.shape, queries.shape)\n",
    "\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "calculate_map(database, labels, queries, query_labels, similarity_sparse_coding)\n",
    "# mAP :  0.13721717999431532\n",
    "# mAP : 0.1581216311119271 with 800\n",
    "print('cost time : ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 64000)\n",
      "generating inverted list with 13113 doc, 64000 interested words.\n"
     ]
    }
   ],
   "source": [
    "# inverted index for ranking\n",
    "import threading\n",
    "import numpy as np\n",
    "# input : database[doc_num, word_dim]\n",
    "# output : inverted_list[word_dim] each entry contains a list for docs index.\n",
    "def generate_inverted_list(database, thread_count = 16):\n",
    "    assert len(database.shape) == 2\n",
    "    num, dim = database.shape\n",
    "    print('generating inverted list with %d doc, %d interested words.' % (num, dim))\n",
    "    inv_list = [[] for i in range(dim)]\n",
    "    \n",
    "    for idx,x in enumerate(database):\n",
    "        for i,j in enumerate(x):\n",
    "            if j == True:\n",
    "                inverted_list[i].append(idx)\n",
    "    return inv_list\n",
    "#output : return an idx_list with decresing similarity which depends on occurrence count in inverted list\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "print(database.shape)\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "inv_list = generate_inverted_list(database)\n",
    "t2 = time.time()\n",
    "print('takes : ', t2-t1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 32000) (120, 32000)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def rank_by_inverted_list(database, inverted_list, query):\n",
    "    assert len(query.shape) == 1\n",
    "    assert len(database.shape) == 2\n",
    "    num, _ = database.shape\n",
    "    count = [0] * num\n",
    "    for i,j in enumerate(query):\n",
    "        if j == True:\n",
    "            for idx in inverted_list[i]:\n",
    "                count[idx] += 1\n",
    "    return reversed(np.argsort(count))\n",
    "# print inv_list\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "labels = data['database_name'].ravel()\n",
    "\n",
    "queries = np.swapaxes(q_codeword, axis1=0, axis2=1)\n",
    "query_labels = data['query_name'].ravel()\n",
    "\n",
    "def similarity_with_inverted_list(codeword, query):\n",
    "    return rank_by_inverted_list(codeword, inv_list, query)\n",
    "\n",
    "\n",
    "calculate_map(database, labels, queries, query_labels, similarity_with_inverted_list)\n",
    "# with open('inv_list.pickle','wb') as f:\n",
    "#     pickle.dump(inv_list, f, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "from l2_distance import l2_distance #l2_distance.py\n",
    "from sparse_coding import sparse_coding,similarity_sparse_coding #sparse_coding.py\n",
    "from calculate_map import calculate_map #calculate_map.py\n",
    "import pickle\n",
    "lfw_path = './hw1_data/LFW_DATA.pickle'\n",
    "with open(lfw_path, 'rb') as f:\n",
    "    lfw = pickle.load(f)\n",
    "database = lfw['database_feature']\n",
    "queries = lfw['query_feature']\n",
    "labels = lfw['database_name'].ravel()\n",
    "query_labels = lfw['query_name'].ravel()\n",
    "\n",
    "#part 1: l2_distance and calculate_map\n",
    "# mAP = calculate_map(database, labels, queries, query_labels, l2_distance)\n",
    "# print(mAP)\n",
    "\n",
    "#part 2: sparse_coding\n",
    "param = { 'K' : 400,\n",
    "                  'lambda1' : 0.01, 'numThreads' : -1, 'mode':2,\n",
    "                  'iter' : -10}\n",
    "        \n",
    "q_sparse, db_sparse = sparse_coding(lfw['query_feature'], lfw['database_feature'], param) #build sparse dict and lookup using spams library\n",
    "sparse_map = calculate_map(db_sparse, labels, q_sparse, query_labels, similarity_sparse_coding)# results_sparse = distance(q_sparse, db_sparse) #you can use l2_distance in part1 or try any distance metric, like cos, l1 \n",
    "print(sparse_map)\n",
    "#part 3\n",
    "lambda1 = [10**i for i in range(-6,-3,1)]\n",
    "K = range(100,3200,400)\n",
    "iter_ = -10\n",
    "for k in K:\n",
    "    for l1 in lambda1:\n",
    "        \n",
    "\n",
    "        param = { 'K' : k,\n",
    "                  'lambda1' : l1, 'numThreads' : -1, 'mode':2,\n",
    "                  'iter' : iter_}\n",
    "        q_sparse, db_sparse = sparse_coding(lfw['query_feature'], lfw['database_feature'], param) #build sparse dict and lookup using spams library\n",
    "        sparse_map = calculate_map(db_sparse, labels, q_sparse, query_labels, similarity_sparse_coding)# results_sparse = distance(q_sparse, db_sparse) #you can use l2_distance in part1 or try any distance metric, like cos, l1 \n",
    "\n",
    "        print(k,l1, sparse_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14895016188476712\n"
     ]
    }
   ],
   "source": [
    "sparse_map = calculate_map(db_sparse, labels, q_sparse, query_labels, similarity_sparse_coding)# results_sparse = distance(q_sparse, db_sparse) #you can use l2_distance in part1 or try any distance metric, like cos, l1 \n",
    "print(sparse_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8452432df8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mD_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_coding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_SC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0msparse_coding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_D_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./dict200.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git_repo/paper_notes/hw/hw1/sparse_coding.py\u001b[0m in \u001b[0;36mtrain_SC\u001b[0;34m(database, param)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathces_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masfortranarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mD_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mD_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/spams.pyc\u001b[0m in \u001b[0;36mtrainDL\u001b[0;34m(X, return_model, model, D, numThreads, batchsize, K, lambda1, lambda2, iter, t0, mode, posAlpha, posD, expand, modeD, whiten, clean, verbose, gamma1, gamma2, rho, iter_updateD, stochastic_deprecated, modeParam, batch, log_deprecated, logName)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[0mlambda3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m     \u001b[0mregul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1983\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m__allTrainDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumThreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposAlpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodeD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_updateD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstochastic_deprecated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodeParam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_deprecated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m def structTrainDL(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/spams.pyc\u001b[0m in \u001b[0;36m__allTrainDL\u001b[0;34m(X, return_model, model, in_memory, D, graph, tree, numThreads, tol, fixed_step, ista, batchsize, K, lambda1, lambda2, lambda3, iter, t0, mode, regul, posAlpha, posD, expand, modeD, whiten, clean, verbose, gamma1, gamma2, rho, iter_updateD, stochastic_deprecated, modeParam, batch, log_deprecated, logName)\u001b[0m\n\u001b[1;32m   1829\u001b[0m         \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposAlpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m         \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodeD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_updateD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m         stochastic_deprecated,modeParam,batch,log_deprecated,logName)\n\u001b[0m\u001b[1;32m   1832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/spams_wrap.pyc\u001b[0m in \u001b[0;36malltrainDL\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0malltrainDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mm_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meta_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgroups_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mown_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVector\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN_own_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mfixed_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mista\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mlambda2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mlambda3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_type\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mname_regul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mposAlpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mposD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_type_D\u001b[0m \u001b[0mmodeD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mwhiten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mgamma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m \u001b[0mgamma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0miter_updateD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mstochastic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mmodeParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogName\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_spams_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malltrainDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marchetypalAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train for attributed\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sparse_coding\n",
    "\n",
    "lfw_path = './hw1_data/LFW_DATA.pickle'\n",
    "with open(lfw_path, 'rb') as f:\n",
    "    lfw = pickle.load(f)\n",
    "database = lfw['database_feature']\n",
    "\n",
    "param = { 'K' : 200,\n",
    "              'lambda1' : 0.01, 'numThreads' : -1, 'mode':2,\n",
    "              'iter' : -10}\n",
    "\n",
    "\n",
    "D_list = sparse_coding.train_SC(database, param)\n",
    "sparse_coding.save_D_list(D_list, out_file='./dict200.npy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load for attributed\n",
    "import pickle\n",
    "import numpy as np\n",
    "lfw_path = './hw1_data/LFW_DATA.pickle'\n",
    "with open(lfw_path, 'rb') as f:\n",
    "    lfw = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qax (120, 1)\n",
      "(59, 80, 120)\n",
      "(16000, 120)\n"
     ]
    }
   ],
   "source": [
    "import spams\n",
    "import scipy\n",
    "\n",
    "def load_D_list(in_file='./dict200.npy'):\n",
    "    with open(in_file,'rb') as f:\n",
    "        data = np.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_name_idx_from_filename(s):\n",
    "    s = s[0].split('\\\\')[-1].split('.jpg')[0]\n",
    "    idx = int(s.split('_')[-1])\n",
    "    buf = s.replace('_',' ').split(' ')[:-1]\n",
    "    \n",
    "    name = ''\n",
    "    for b in buf:\n",
    "        name += b\n",
    "        name += ' '\n",
    "    name = name.strip()\n",
    "    return name, idx\n",
    "\n",
    "def get_attribute_martix(features, identities, attribute_dict):\n",
    "    X = np.zeros([ features.shape[0], 1])\n",
    "    for i,f in enumerate(identities):\n",
    "        name, idx = extract_name_idx_from_filename(f)\n",
    "        value = 0\n",
    "        if not attribute_dict.has_key(name):\n",
    "            value = 0\n",
    "        elif not attribute_dict[name].has_key(idx):\n",
    "            value = 0\n",
    "        else:\n",
    "            value = attribute_dict[name][idx]\n",
    "        X[i,0] = value\n",
    "    return X\n",
    "\n",
    "def get_weight_matrix(att, p=200, weight=float('inf')):\n",
    "#     W:  double p x n matrix   (weights)\n",
    "    n = att.shape[0]\n",
    "    W = np.ones([p,n])\n",
    "    for i,at in enumerate(att):\n",
    "        if at >= 0: # contain missing value\n",
    "            W[:p/2] = weight\n",
    "        else:\n",
    "            W[p/2:] = weight\n",
    "    return np.asfortranarray(W)\n",
    "\n",
    "def load_attribute_dict(path='./hw1_data/lfw_attributes.txt'):\n",
    "    # prepare dict for attributes with name\n",
    "    with open(path) as f:\n",
    "        ls = f.readlines()\n",
    "    at = ls[2:]\n",
    "    attribute_dict = dict()\n",
    "    for l in at:\n",
    "        atts = l.strip().split('\\t')\n",
    "    #     print atts\n",
    "        name,idx,_,_,white = atts[0:5]\n",
    "        if not attribute_dict.has_key(name):\n",
    "            attribute_dict[name] = dict()\n",
    "        attribute_dict[name][int(idx)] = float(white)\n",
    "    return attribute_dict\n",
    "\n",
    "def get_codeword(D_list, X, aX, lambda1=0.01, pathces_num=80):\n",
    "    \n",
    "    param = { 'lambda1' : lambda1, 'numThreads' : -1, 'mode':2}\n",
    "    buf = []\n",
    "    for i in range(pathces_num):\n",
    "        q = np.asfortranarray(X[:,i,:])\n",
    "        D = D_list[i]\n",
    "        W = get_weight_matrix(aX)\n",
    "        a = spams.lassoWeighted(q,D=np.asfortranarray(D),W=W,**param)\n",
    "        buf.append(a)\n",
    "    buf = scipy.sparse.vstack(buf)\n",
    "    codeword = np.zeros(buf.shape, dtype=np.bool)\n",
    "    codeword[buf.nonzero()] = True\n",
    "    return codeword\n",
    "\n",
    "database = lfw['database_feature']\n",
    "file_names = lfw['database_identity'].ravel()\n",
    "q_database = lfw['query_feature']\n",
    "q_file_names = lfw['query_identity'].ravel()\n",
    "\n",
    "attribute_dict = load_attribute_dict(path='./hw1_data/lfw_attributes.txt')\n",
    "\n",
    "aX = get_attribute_martix(database, file_names, attribute_dict)\n",
    "qaX = get_attribute_martix(q_database, q_file_names, attribute_dict)\n",
    "print 'qax',qaX.shape\n",
    "D_list = load_D_list('./dict200.npy')\n",
    "\n",
    "\n",
    "query = lfw['query_feature']\n",
    "query = query.reshape([-1,80,59])\n",
    "query = np.swapaxes(query, 0, 2)\n",
    "print query.shape\n",
    "c = get_codeword(D_list, query, qaX)\n",
    "print c.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
