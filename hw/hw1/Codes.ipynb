{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./hw1_data/data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#print data.keys()\n",
    "#[u'database_name', u'query_identity', u'database_identity', u'query_name', u'database_feature', u'query_feature']\n",
    "src = data['database_feature']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read attributes\n",
    "with open('./hw1_data/lfw_attributes.txt') as f:\n",
    "    attributes = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_distance.py\n",
    "import numpy as np\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# return index list with decreasing similarity\n",
    "def l2_distance(database, query):\n",
    "    q = query.ravel()\n",
    "    \n",
    "    #check feature dimension\n",
    "    assert len(q) == 4720\n",
    "    assert len(database.shape) == 2 and database.shape[1] == 4720\n",
    "    distances = []\n",
    "    for x in database:\n",
    "        distances.append(np.linalg.norm(x-q,ord=2))\n",
    "    return np.argsort(distances)\n",
    "\n",
    "\n",
    "# calculate_map.py\n",
    "def ap(idx_list, y):\n",
    "    hit = 0\n",
    "    n = 0\n",
    "    precisions = []\n",
    "    true_label_count = sum(y)\n",
    "    for i in idx_list:\n",
    "        y_true = y[i]\n",
    "        n +=1\n",
    "        if y_true == 1:\n",
    "            hit += 1\n",
    "            \n",
    "            precisions.append(float(hit)/n)\n",
    "            if hit == true_label_count:\n",
    "                break\n",
    "    ap = np.sum(precisions) / true_label_count\n",
    "    return ap\n",
    "\n",
    "def calculate_map(database, labels, queries, query_labels, similarity_func):\n",
    "    APs = [] \n",
    "    i = 1\n",
    "    for query,label in zip(queries,query_labels):\n",
    "        idx_list = similarity_func(database, query)\n",
    "        y = [1 if l == label else 0 for l in labels]\n",
    "        APs.append(ap(idx_list, y))\n",
    "#         print i\n",
    "        i+=1\n",
    "    mAP = np.mean(APs)\n",
    "    print 'mAP : ', mAP \n",
    "    \n",
    "database = data['database_feature']\n",
    "labels = data['database_name'].ravel()\n",
    "\n",
    "queries = data['query_feature']\n",
    "query_labels = data['query_name'].ravel()\n",
    "# calculate_map(database, labels, queries, query_labels, l2_distance)\n",
    "# print'mAP :  0.10249282626788106'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 4720)\n",
      "(59, 80, 13113)\n"
     ]
    }
   ],
   "source": [
    "import spams\n",
    "import numpy as np\n",
    "print src.shape\n",
    "num = src.shape[0]\n",
    "X = src.reshape([num, 80,59])\n",
    "\n",
    "X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "\n",
    "# X = src.reshape([num, 59,80])\n",
    "# X = np.swapaxes(X,axis1=0,axis2=1)\n",
    "# X = np.swapaxes(X,axis1=1,axis2=2)\n",
    "lambda1 = [10**i for i in range(-6,-3,1)]\n",
    "K = range(100,3200,400)\n",
    "param = { 'K' : 400, # learns a dictionary with 100 elements\n",
    "          'lambda1' : 10**-2, 'numThreads' : -1, 'mode':2,\n",
    "          'iter' : 1000}\n",
    "\n",
    "    \n",
    "print X.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 4720)\n",
      "(59, 80, 13113)\n",
      "(32000, 120)\n",
      "(32000, 13113)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import spams\n",
    "# print D\n",
    "pathces_num = 80\n",
    "test = data['query_feature']\n",
    "num = test.shape[0]\n",
    "qs = test.reshape([num, 80,59])\n",
    "qs = np.swapaxes(qs,axis1=0,axis2=2)\n",
    "# \n",
    "with open('./dict.npy','rb') as f:\n",
    "    D_list = np.load(f)\n",
    "    D_list = [np.asfortranarray(D) for D in D_list]\n",
    "def get_codeword(lambda1,D_list, X):\n",
    "    \n",
    "    param = { 'lambda1' : lambda1, 'lambda2' : 0,'numThreads' : -1, 'mode':2}\n",
    "    buf = []\n",
    "    for i in range(pathces_num):\n",
    "        q = np.asfortranarray(X[:,i,:])\n",
    "        D = D_list[i]\n",
    "        a = spams.lasso(q,D=D,return_reg_path = False,**param)\n",
    "        buf.append(a)\n",
    "    buf = scipy.sparse.vstack(buf)\n",
    "    codeword = np.zeros(buf.shape, dtype=np.bool)\n",
    "    codeword[buf.nonzero()] = True\n",
    "    return codeword\n",
    "\n",
    "#\n",
    "print src.shape\n",
    "num = src.shape[0]\n",
    "X = src.reshape([num, 80,59])\n",
    "\n",
    "X = np.swapaxes(X,axis1=0,axis2=2)\n",
    "print X.shape\n",
    "# \n",
    "q_codeword = get_codeword(10**-2, D_list, qs)\n",
    "codeword = get_codeword(10**-2, D_list, X)\n",
    "print q_codeword.shape\n",
    "print codeword.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codeword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c7a791e577a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodeword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'database_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codeword' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# 注意要計算and而不是xor(hamming distance)\n",
    "def similarity_sparse_coding(codeword, query):\n",
    "    q = query.ravel()\n",
    "    \n",
    "    distances = []\n",
    "    for x in codeword:\n",
    "#         distances.append(scipy.spatial.distance.hamming(x,query))\n",
    "        distances.append(np.sum(np.logical_and(x,q)))\n",
    "    return reversed(np.argsort(distances))\n",
    "\n",
    "\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "labels = data['database_name'].ravel()\n",
    "\n",
    "queries = np.swapaxes(q_codeword, axis1=0, axis2=1)\n",
    "query_labels = data['query_name'].ravel()\n",
    "print database.shape, queries.shape\n",
    "\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "calculate_map(database, labels, queries, query_labels, similarity_sparse_coding)\n",
    "# mAP :  0.13721717999431532\n",
    "print 'cost time : ', time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 32000)\n",
      "generating inverted list with 13113 doc, 32000 interested words.\n",
      "takes :  326.134527206\n",
      "generating inverted list with 13113 doc, 32000 interested words.\n",
      "takes :  375.934621811\n"
     ]
    }
   ],
   "source": [
    "# inverted index for ranking\n",
    "import threading\n",
    "import numpy as np\n",
    "# input : database[doc_num, word_dim]\n",
    "# output : inverted_list[word_dim] each entry contains a list for docs index.\n",
    "def generate_inverted_list(database, thread_count = 16):\n",
    "    assert len(database.shape) == 2\n",
    "    num, dim = database.shape\n",
    "    print 'generating inverted list with %d doc, %d interested words.' % (num, dim)\n",
    "    inv_list = [[] for i in range(dim)]\n",
    "    \n",
    "    def count_occurrence(database, dim, interval, inv_list, semaphore):\n",
    "        X = database[:,dim: dim+interval]\n",
    "        for idx in range(len(database)):\n",
    "            for i in range(interval):\n",
    "                if X[idx,i] == True:\n",
    "                    inv_list[dim+1].append(idx)\n",
    "        semaphore.release()\n",
    "    \n",
    "    semaphore = threading.Semaphore(value=thread_count)\n",
    "    \n",
    "    interval = 1000\n",
    "    for d in range(0,dim, interval):\n",
    "        semaphore.acquire()\n",
    "        t = threading.Thread(target=count_occurrence, args=(database, d, interval, inv_list, semaphore), name='count_dim%d'%d )\n",
    "        t.start()\n",
    "    \n",
    "#     for idx,x in enumerate(database):\n",
    "#         for i,j in enumerate(x):\n",
    "#             if j == True:\n",
    "#                 inverted_list[i].append(idx)\n",
    "    return inv_list\n",
    "#output : return an idx_list with decresing similarity which depends on occurrence count in inverted list\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "# database = database[:3000,:]\n",
    "print database.shape\n",
    "import time\n",
    "t1 = time.time()\n",
    "inv_list = generate_inverted_list(database)\n",
    "t2 = time.time()\n",
    "print 'takes : ', t2-t1\n",
    "\n",
    "def generate_inverted_list2(database):\n",
    "    assert len(database.shape) == 2\n",
    "    num, dim = database.shape\n",
    "    print 'generating inverted list with %d doc, %d interested words.' % (num, dim)\n",
    "    inv_list = [[] for i in range(dim)]\n",
    "    \n",
    "    for idx,x in enumerate(database):\n",
    "        for i,j in enumerate(x):\n",
    "            if j == True:\n",
    "                inv_list[i].append(idx)\n",
    "    return inv_list\n",
    "inv_list2 = generate_inverted_list2(database)\n",
    "t3 = time.time()\n",
    "print 'takes : ', t3-t2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13113, 32000) (120, 32000)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "q = queries[0,:]\n",
    "\n",
    "def rank_by_inverted_list(database, inverted_list, query):\n",
    "    assert len(query.shape) == 1\n",
    "    assert len(database.shape) == 2\n",
    "    num, _ = database.shape\n",
    "    count = [0] * num\n",
    "    for i,j in enumerate(query):\n",
    "        if j == True:\n",
    "            for idx in inverted_list[i]:\n",
    "                count[idx] += 1\n",
    "    return reversed(np.argsort(count))\n",
    "# print inv_list\n",
    "database = np.swapaxes(codeword, axis1=0, axis2=1)\n",
    "labels = data['database_name'].ravel()\n",
    "\n",
    "queries = np.swapaxes(q_codeword, axis1=0, axis2=1)\n",
    "query_labels = data['query_name'].ravel()\n",
    "print database.shape, queries.shape\n",
    "\n",
    "def similarity_with_inverted_list(codeword, query):\n",
    "    return rank_by_inverted_list(codeword, inv_list, query)\n",
    "\n",
    "\n",
    "# calculate_map(database, labels, queries, query_labels, similarity_with_inverted_list)\n",
    "with open('inv_list.pickle','wb') as f:\n",
    "    pickle.dump(inv_list, f, protocol=2)\n",
    "print 'Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('inv_list.pickle','rb') as f:\n",
    "    inv_list = pickle.load(f)\n",
    "print type(inv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle transform from protocol 3 to 2\n",
    "# Must be kernel python3\n",
    "import pickle\n",
    "def trans(src, dst):\n",
    "    with open(src,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    with open(dst,'wb') as f:\n",
    "        pickle.dump(data, f, protocol=2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
