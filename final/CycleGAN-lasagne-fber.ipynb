{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasagne implementation of https://junyanz.github.io/CycleGAN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run,dnn.library_path=/usr/bin,KERAS_BACKEND=theano '\n",
    "os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run,dnn.library_path=/usr/bin'\n",
    "\n",
    "# os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run'\n",
    "# os.environ['THEANO_FLAGS']='device=cuda'\n",
    "\n",
    "\n",
    "channel_first = True\n",
    "channel_axis=1\n",
    "# import pygpu\n",
    "# print pygpu.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7005 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 970 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne.layers import DropoutLayer, ReshapeLayer, InputLayer \n",
    "floatX = theano.config.floatX\n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, ConcatLayer, NonlinearityLayer\n",
    "from lasagne.layers import batch_norm\n",
    "from lasagne.nonlinearities import LeakyRectify, sigmoid, rectify, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_init = lasagne.init.Normal(0.02, 0)\n",
    "gamma_init = lasagne.init.Normal(0.02, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BASIC_D(nc_in, ndf, max_layers=3, use_sigmoid=True):\n",
    "    l=-1\n",
    "    def conv2d(x, nf, l, stride=2, nonlinearity=LeakyRectify(0.2)):\n",
    "#         nonlocal l\n",
    "        l+=1\n",
    "        return l,Conv2DLayer(x, num_filters=nf, filter_size=4, stride=stride, \n",
    "                           pad=1, W=conv_init, flip_filters=False,\n",
    "                            nonlinearity=nonlinearity,\n",
    "                           name=\"conv2d_{}\".format(l)\n",
    "                          )\n",
    "    _ = InputLayer(shape=(None, nc_in, None, None), name=\"input\")\n",
    "    l, _ = conv2d(_, ndf, l)\n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        l,_ = conv2d(_, out_feat, l)\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    l,_ = conv2d(_, out_feat, l, stride=1)\n",
    "    _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)         \n",
    "    l,_ = conv2d(_, 1, l, stride=1, nonlinearity=sigmoid if use_sigmoid else None)\n",
    "    return _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://gist.github.com/ajbrock/a3858c26282d9731191901b397b3ce9f\n",
    "def reflect_pad(x, width, batch_ndim=1):\n",
    "    \"\"\"\n",
    "    Pad a tensor with a constant value.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensor\n",
    "    width : int, iterable of int, or iterable of tuple\n",
    "        Padding width. If an int, pads each axis symmetrically with the same\n",
    "        amount in the beginning and end. If an iterable of int, defines the\n",
    "        symmetric padding width separately for each axis. If an iterable of\n",
    "        tuples of two ints, defines a seperate padding width for each beginning\n",
    "        and end of each axis.\n",
    "    batch_ndim : integer\n",
    "        Dimensions before the value will not be padded.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Idea for how to make this happen: Flip the tensor horizontally to grab horizontal values, then vertically to grab vertical values\n",
    "    # alternatively, just slice correctly\n",
    "    input_shape = x.shape\n",
    "    input_ndim = x.ndim\n",
    "\n",
    "    output_shape = list(input_shape)\n",
    "    indices = [slice(None) for _ in output_shape]\n",
    "\n",
    "    if isinstance(width, int):\n",
    "        widths = [width] * (input_ndim - batch_ndim)\n",
    "    else:\n",
    "        widths = width\n",
    "\n",
    "    for k, w in enumerate(widths):\n",
    "        try:\n",
    "            l, r = w\n",
    "        except TypeError:\n",
    "            l = r = w\n",
    "        output_shape[k + batch_ndim] += l + r\n",
    "        indices[k + batch_ndim] = slice(l, l + input_shape[k + batch_ndim])\n",
    "\n",
    "    # Create output array\n",
    "    out = T.zeros(output_shape)\n",
    "    \n",
    "    # Vertical Reflections\n",
    "    out=T.set_subtensor(out[:,:,:width,width:-width], x[:,:,width:0:-1,:])# out[:,:,:width,width:-width] = x[:,:,width:0:-1,:]\n",
    "    out=T.set_subtensor(out[:,:,-width:,width:-width], x[:,:,-2:-(2+width):-1,:])#out[:,:,-width:,width:-width] = x[:,:,-2:-(2+width):-1,:]\n",
    "    \n",
    "    # Place X in out\n",
    "    # out = T.set_subtensor(out[tuple(indices)], x) # or, alternative, out[width:-width,width:-width] = x\n",
    "    out=T.set_subtensor(out[:,:,width:-width,width:-width],x)#out[:,:,width:-width,width:-width] = x\n",
    "   \n",
    "   #Horizontal reflections\n",
    "    out=T.set_subtensor(out[:,:,:,:width],out[:,:,:,(2*width):width:-1])#out[:,:,:,:width] = out[:,:,:,(2*width):width:-1]\n",
    "    out=T.set_subtensor(out[:,:,:,-width:],out[:,:,:,-(width+2):-(2*width+2):-1])#out[:,:,:,-width:] = out[:,:,:,-(width+2):-(2*width+2):-1]\n",
    "    \n",
    "    \n",
    "    return out\n",
    "    \n",
    "class ReflectLayer(lasagne.layers.Layer):\n",
    "\n",
    "    def __init__(self, incoming, width, batch_ndim=2, **kwargs):\n",
    "        super(ReflectLayer, self).__init__(incoming, **kwargs)\n",
    "        self.width = width\n",
    "        self.batch_ndim = batch_ndim\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "\n",
    "        if isinstance(self.width, int):\n",
    "            widths = [self.width] * (len(input_shape) - self.batch_ndim)\n",
    "        else:\n",
    "            widths = self.width\n",
    "\n",
    "        for k, w in enumerate(widths):\n",
    "            if output_shape[k + self.batch_ndim] is None:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    l, r = w\n",
    "                except TypeError:\n",
    "                    l = r = w\n",
    "                output_shape[k + self.batch_ndim] += l + r\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return reflect_pad(input, self.width, self.batch_ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=64, fixed_input_size=True):    \n",
    "    max_nf = 8*ngf    \n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "            \n",
    "        x = Conv2DLayer(x, num_filters=nf_next, filter_size=4, stride=2, pad=1, W=conv_init, flip_filters=False,                \n",
    "                nonlinearity=None, name='conv2d_{}'.format(s))\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batch_norm(x, epsilon=1e-5, gamma=gamma_init)\n",
    "            x2 = NonlinearityLayer(x, nonlinearity=LeakyRectify(0.2), name=\"leakyRelu_{}\".format(s))\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = ConcatLayer([x, x2], name=\"concat_{}\".format(s))            \n",
    "        x = NonlinearityLayer(x, nonlinearity=rectify, name=\"Relu_{}\".format(s))\n",
    "        x = TransposedConv2DLayer(x, num_filters=nf_out, filter_size=4, stride=2, crop=1, W=conv_init, \n",
    "                                  flip_filters=True, nonlinearity=None, name=\"convt_{}\".format(s))\n",
    "        if use_batchnorm:\n",
    "            x = batch_norm(x, epsilon=1e-5, gamma=gamma_init)\n",
    "        if s <= 8:\n",
    "            x = DropoutLayer(x, 0.5, name=\"dropout_{}\".format(s))\n",
    "        return x\n",
    "    \n",
    "    s = isize if fixed_input_size else None\n",
    "    _ = InputLayer(shape=(None, nc_in, s, s), name='input')\n",
    "    _ = block(_, isize, nc_in, False, nf_out=nc_out, nf_next=ngf)\n",
    "    _ = NonlinearityLayer(_, nonlinearity=tanh, name='tanh')\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne.layers import ElemwiseSumLayer, SliceLayer\n",
    "def reflect_padding_conv(_, num_filters, filter_size=3, stride=1, nonlinearity=rectify, use_batchnorm=True, **k):\n",
    "    assert filter_size%2==1\n",
    "    pad_size = filter_size>>1\n",
    "    _ = ReflectLayer(_, width=pad_size)\n",
    "    _ = Conv2DLayer(_, num_filters=num_filters, filter_size=filter_size, stride=stride, \n",
    "                           pad=0, W=conv_init, flip_filters=False, nonlinearity=nonlinearity, **k)\n",
    "    if use_batchnorm:\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    return _\n",
    "def res_block(_, num_filters, name):\n",
    "    x = _\n",
    "    _ = reflect_padding_conv(_, num_filters, name=name+\"_conv1\")\n",
    "    _ = reflect_padding_conv(_, num_filters, nonlinearity=None, name=name+\"_conv2\")\n",
    "    return ElemwiseSumLayer([x, _], name=name+\"_add\")\n",
    "\n",
    "def RESNET_G(nc_in=3, nc_out=3, ngf=64, fixed_input_size=True):\n",
    "    s = None\n",
    "    _ = InputLayer(shape=(None, nc_in, s, s), name='input')    \n",
    "    _ = reflect_padding_conv(_, ngf, 7, name=\"first\")    \n",
    "    for m in [2,4]:\n",
    "        _ = Conv2DLayer(_, num_filters=ngf*m, filter_size=4, stride=2, \n",
    "                           pad=1, W=conv_init, flip_filters=False, \n",
    "                           nonlinearity=rectify, name='conv_{}'.format(ngf*m))\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    for i in range(6):\n",
    "        _ = res_block(_, ngf*4, \"res_block{}\".format(i))\n",
    "    for m in [2,1]:\n",
    "        _ = TransposedConv2DLayer(_, num_filters=ngf*m, filter_size=3, stride=2, \n",
    "                            crop=0, W=conv_init, flip_filters=True,\n",
    "                            nonlinearity=rectify, name=\"convt_{}\".format(ngf*m))\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "        _ = SliceLayer(_, slice(0, -1),2)\n",
    "        _ = SliceLayer(_, slice(0, -1),3)    \n",
    "    _ = reflect_padding_conv(_, nc_out, 7, nonlinearity=tanh, use_batchnorm=False, name=\"output\")\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "use_lsgan = True\n",
    "#λ = 10 if use_lsgan else 100\n",
    "lambda_ = 10\n",
    "\n",
    "# loadSize = 143\n",
    "# loadSize = 286\n",
    "# imageSize = 256\n",
    "loadSize = 143\n",
    "imageSize = 128\n",
    "\n",
    "# imageSizeX, imageSizeY = 128, 192\n",
    "batchSize = 1\n",
    "lrD = 5e-5\n",
    "lrG = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "netDA = BASIC_D(nc_in, ndf, use_sigmoid = not use_lsgan)\n",
    "netDB = BASIC_D(nc_out, ndf, use_sigmoid = not use_lsgan)\n",
    "# for l in lasagne.layers.get_all_layers(netDA):\n",
    "#     print(l.name,  l.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netG = UNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "netGB = RESNET_G(nc_in, nc_out, ngf)\n",
    "netGA = RESNET_G(nc_out, nc_in, ngf)\n",
    "# netGA.summary()\n",
    "# for l in lasagne.layers.get_all_layers(netGA):\n",
    "#     print(l.name,  l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne.layers import get_output, get_all_layers,get_all_params\n",
    "no_bn_avg = dict( batch_norm_update_averages=False, batch_norm_use_averages=False)\n",
    "\n",
    "if use_lsgan:\n",
    "    loss_fn = lambda output, target : T.mean( (output-target)**2 )\n",
    "else:\n",
    "    loss_fn = lambda output, target : -T.mean(T.log(output+1e-12)*target+T.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "def cycle_variables(netG1, netG2):\n",
    "    real_input = get_all_layers(netG1)[0].input_var    \n",
    "    fake_output = get_output(netG1, **no_bn_avg)\n",
    "    rec_input = get_output(netG2, inputs=fake_output)\n",
    "    fn_generate = theano.function([real_input], [fake_output, rec_input])\n",
    "    return real_input, fake_output, rec_input, fn_generate\n",
    "\n",
    "real_A, fake_B, rec_A, cycleA_generate = cycle_variables(netGB, netGA)\n",
    "real_B, fake_A, rec_B, cycleB_generate = cycle_variables(netGA, netGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_loss(netD, real, fake, rec):\n",
    "    output_real = get_output(netD, inputs=real, **no_bn_avg)\n",
    "    output_fake = get_output(netD, inputs=fake, **no_bn_avg)\n",
    "    loss_D_real = loss_fn(output_real, T.ones_like(output_real))\n",
    "    loss_D_fake = loss_fn(output_fake, T.zeros_like(output_fake))\n",
    "    loss_G = loss_fn(output_fake, T.ones_like(output_fake))\n",
    "    loss_D = loss_D_real+loss_D_fake\n",
    "    loss_cyc = T.mean(abs(rec-real))\n",
    "    return loss_D, loss_G, loss_cyc\n",
    "\n",
    "loss_DA, loss_GA, loss_cycA = D_loss(netDA, real_A, fake_A, rec_A)\n",
    "loss_DB, loss_GB, loss_cycB = D_loss(netDB, real_B, fake_B, rec_B)\n",
    "loss_cyc = loss_cycA+loss_cycB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "params_netD = get_all_params([netDA, netDB], trainable=True)\n",
    "params_netG = get_all_params([netGA, netGB], trainable=True)\n",
    "\n",
    "loss_G = loss_GA+loss_GB+lambda_*loss_cyc\n",
    "loss_D = loss_DA+loss_DB\n",
    "\n",
    "\n",
    "# netD_opt = Adam(lr=lrD, beta_1=0.5)\n",
    "# training_updates = netD_opt.get_updates(params_netD,[],loss_D)\n",
    "# netD_train = K.function([real_A, real_B],[loss_DA/2, loss_DB/2], updates=training_updates)\n",
    "\n",
    "# netG_opt = Adam(lr=lrG, beta_1=0.5)\n",
    "# training_updates = netG_opt.get_updates(params_netG,[], loss_G)\n",
    "# netG_train = K.function([real_A, real_B], [loss_GA, loss_GB, loss_cyc], updates=training_updates)\n",
    "\n",
    "optimize_D = lasagne.updates.adam(loss_D, params_netD, learning_rate=lrD, beta1=0.5)\n",
    "netD_train = theano.function([real_A, real_B], [loss_DA/2, loss_DB/2], updates=optimize_D)\n",
    "\n",
    "optimize_G = lasagne.updates.adam(loss_G, params_netG, learning_rate=lrG, beta1=0.5)\n",
    "netG_train =  theano.function([real_A, real_B], [loss_GA, loss_GB, loss_cyc], updates=optimize_G)\n",
    "\n",
    "clamp_lower, clamp_upper = -0.02, 0.02\n",
    "mn =lambda w: 1 if w.name.endswith('gamma') else 0\n",
    "clamp_D_fn = theano.function([], [], updates = [(w, T.clip(w, mn(w)+clamp_lower, mn(w)+clamp_upper)) for w in params_netD])\n",
    "# clamp_D_fn = K.function([], [], updates = [(w, T.clip(w, mn(w)+clamp_lower, mn(w)+clamp_upper)) for w in params_netD])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "\n",
    "def read_image(fn):\n",
    "#     load image with BILINEAR resizing, and crop the central region. 143*143 -> 128*128\n",
    "    im = Image.open(fn).convert('RGB')\n",
    "    im = im.resize( (loadSize, loadSize), Image.BILINEAR )\n",
    "#     normalize,[0,255] -> [-1,1]\n",
    "    arr = np.array(im)/255. * 2 -1\n",
    "#     print 'im',im\n",
    "#     print arr\n",
    "    w1,w2 = (loadSize-imageSize)//2,(loadSize+imageSize)//2\n",
    "    h1,h2 = w1,w2\n",
    "    img = arr[h1:h2, w1:w2, :]\n",
    "#     horizontal flip\n",
    "    if randint(0,1):\n",
    "        img=img[:,::-1]\n",
    "    if channel_first:        \n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "    return img\n",
    "\n",
    "#data = \"edges2shoes\"\n",
    "# dataset = \"horse2zebra\"\n",
    "# dataset = \"face2emoji\"\n",
    "# dataset = 'face2president'\n",
    "dataset = \"president2coach\"\n",
    "# dataset = 'annie2coach'\n",
    "# dataset = \"lfw2coach\"\n",
    "# dataset = \"hao2lin\"\n",
    "train_A = load_data('CycleGAN/{}/trainA/*.jpg'.format(dataset))\n",
    "train_B = load_data('CycleGAN/{}/trainB/*.jpg'.format(dataset))\n",
    "test_A = load_data('CycleGAN/{}/testA/*.jpg'.format(dataset))\n",
    "test_B = load_data('CycleGAN/{}/testB/*.jpg'.format(dataset))\n",
    "assert len(train_A) and len(train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data, batchsize):\n",
    "    length = len(data)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    shuffle(data)\n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(data)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        rtn = [read_image(data[j]) for j in range(i,i+size)]\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, np.float32(rtn)       \n",
    "\n",
    "def minibatchAB(dataA, dataB, batchsize):\n",
    "    batchA=minibatch(dataA, batchsize)\n",
    "    batchB=minibatch(dataB, batchsize)\n",
    "    tmpsize = None    \n",
    "    while True:        \n",
    "        ep1, A = batchA.send(tmpsize)\n",
    "        ep2, B = batchB.send(tmpsize)\n",
    "        tmpsize = yield max(ep1, ep2), A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    if channel_first:\n",
    "        int_X = np.moveaxis(int_X.reshape(-1,3,imageSize,imageSize), 1, 3)\n",
    "    else:\n",
    "        int_X = int_X.reshape(-1,imageSize,imageSize, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "#     \n",
    "#     im = im.resize( (loadSize, loadSize), Image.BILINEAR )\n",
    "    display(Image.fromarray(int_X))\n",
    "    return int_X\n",
    "\n",
    "# \n",
    "train_batch = minibatchAB(train_A, train_B, 6)\n",
    "_, A, B = next(train_batch)\n",
    "print A.shape\n",
    "# print A\n",
    "showX(A)\n",
    "showX(B)\n",
    "del train_batch, A, B\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showG(A,B):\n",
    "    assert A.shape==B.shape\n",
    "    def G(fn_generate, X):\n",
    "        r = np.array([fn_generate(X[i:i+1]) for i in range(X.shape[0])]).swapaxes(0,1)[:,:,0]\n",
    "        return r\n",
    "    rA = G(cycleA_generate, A)\n",
    "    rB = G(cycleB_generate, B)\n",
    "    arr = np.concatenate([A,B,rA[0],rB[0],rA[1],rB[1]])\n",
    "    return showX(arr, 3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_model_tmp(path):\n",
    "    v = lasagne.layers.get_all_param_values([netDA, netDB, netGA, netGB])\n",
    "    np.savez('./tmp.npz', *v)\n",
    "    np.savez(path, *v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def save(dataset_name):\n",
    "    # save models\n",
    "    \n",
    "    dir_path = join('./','models',dataset_name)\n",
    "    if not os.path.isdir(join('./','models')):\n",
    "        os.mkdir(join('./','models'))\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        \n",
    "\n",
    "    save_model_tmp(join(dir_path,'%s.npz' % dataset_name))\n",
    "    \n",
    "    \n",
    "\n",
    "# def load(dataset_name, with_opt):\n",
    "#     # load model\n",
    "    \n",
    "#     print 'load model from %s, with optimizer weights : %s' % (dataset_name, with_opt)\n",
    "    \n",
    "#     dir_path = join('./','models',dataset_name)\n",
    "    \n",
    "#     netDA.load_weights(join(dir_path,'netDA.hdf5'))\n",
    "#     netDB.load_weights(join(dir_path,'netDB.hdf5'))\n",
    "#     netGA.load_weights(join(dir_path,'netGA.hdf5'))\n",
    "#     netGB.load_weights(join(dir_path,'netGB.hdf5'))\n",
    "# #     netDA = load_model(join(dir_path,'netDA.hdf5'))\n",
    "# #     netDB = load_model(join(dir_path,'netDB.hdf5'))\n",
    "# #     netGA = load_model(join(dir_path,'netGA.hdf5'))\n",
    "# #     netGB = load_model(join(dir_path,'netGB.hdf5'))\n",
    "#     if with_opt:\n",
    "#         training_optimizers = [netD_opt, netG_opt]\n",
    "#         load_optimizers(training_optimizers, dataset_name)\n",
    "    \n",
    "# def save_optimizers(opt_list, dataset_name):\n",
    "#     netD_opt, netG_opt = opt_list\n",
    "#     root_path = make_path(join('./','models',dataset,'Adam_opt'))\n",
    "    \n",
    "#     opt_path = make_path(join(root_path, 'netD_opt'))\n",
    "#     save_opt_weights(opt_path, netD_opt)\n",
    "    \n",
    "#     opt_path = make_path(join(root_path, 'netG_opt'))\n",
    "#     save_opt_weights(opt_path, netG_opt)\n",
    "    \n",
    "    \n",
    "# def load_optimizers(opt_list, dataset_name):\n",
    "#     netD_opt, netG_opt = opt_list\n",
    "#     root_path = join('./','models',dataset,'Adam_opt')\n",
    "\n",
    "#     opt_path = join(root_path, 'netD_opt')\n",
    "#     load_opt_weights(opt_path, netD_opt)\n",
    "#     opt_path = join(root_path, 'netG_opt')\n",
    "#     load_opt_weights(opt_path, netG_opt)\n",
    "    \n",
    "    \n",
    "# def make_path(path):\n",
    "#     if not os.path.isdir(path):\n",
    "#         os.mkdir(path)\n",
    "#     return path\n",
    "\n",
    "\n",
    "# def save_opt_weights(path, opt):\n",
    "#     wei = opt.get_weights()\n",
    "#     for i,w in enumerate(wei):\n",
    "#         out_path = join(path,'%d.opt' % i)\n",
    "#         with open(out_path, 'wb') as f:\n",
    "#             np.save(f, w)\n",
    "# def load_opt_weights(path, opt):\n",
    "#     N =  len([f for f in os.listdir(path) if f.endswith('.opt')])\n",
    "#     wei = []\n",
    "#     for i in range(N):\n",
    "#         out_path = join(path,'%d.opt' % i)\n",
    "#         with open(out_path, 'rb') as f:\n",
    "#             w = np.load(f)\n",
    "#         wei.append(w)\n",
    "#     opt.set_weights(wei)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "niter = 150\n",
    "img_dir = join('.','buf_data','imgs')\n",
    "gen_iterations = 0\n",
    "epoch = 0\n",
    "errCyc_sum = errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "\n",
    "display_iters = 100\n",
    "#val_batch = minibatch(valAB, 6, direction)\n",
    "train_batch = minibatchAB(train_A, train_B, batchSize)\n",
    "\n",
    "while epoch < niter:\n",
    "    epoch, A, B = next(train_batch)        \n",
    "    #clamp_D_fn()\n",
    "    errDA, errDB  = netD_train(A, B)\n",
    "    errDA_sum +=errDA\n",
    "    errDB_sum +=errDB\n",
    "\n",
    "    # epoch, trainA, trainB = next(train_batch)\n",
    "    errGA, errGB, errCyc = netG_train(A, B)\n",
    "    errGA_sum += errGA\n",
    "    errGB_sum += errGB\n",
    "    errCyc_sum += errCyc\n",
    "    gen_iterations+=1\n",
    "    if gen_iterations%display_iters==0:\n",
    "        #if gen_iterations%(5*display_iters)==0:\n",
    "        clear_output()\n",
    "        print('[%d/%d][%d] Loss_D: %f %f Loss_G: %f %f loss_cyc %f'\n",
    "        % (epoch, niter, gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "           errGA_sum/display_iters, errGB_sum/display_iters, \n",
    "           errCyc_sum/display_iters), time.time()-t0)\n",
    "        _, A, B = train_batch.send(4)\n",
    "        test_sample = showG(A,B)        \n",
    "        img = Image.fromarray(test_sample)\n",
    "        img.save(join(img_dir,'test_%d.jpg' % gen_iterations))\n",
    "        save(dataset)\n",
    "        errCyc_sum = errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "    if gen_iterations%20000==0:\n",
    "        save(dataset+'str(gen_iterations))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = lasagne.layers.get_all_param_values([netDA, netDB, netGA, netGB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('model_cycle_fber2.npz', *v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 192, 128)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveX(fn, X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    if channel_first:\n",
    "        int_X = np.moveaxis(int_X.reshape(-1,3,imageSizeY,imageSizeX), 1, 3)\n",
    "    else:\n",
    "        int_X = int_X.reshape(-1,imageSizeY,imageSizeX, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSizeY, imageSizeX,3).swapaxes(1,2).reshape(rows*imageSizeY,-1, 3)    \n",
    "    Image.fromarray(int_X).save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7202): #for i in range(19000,31000):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    im = Image.open('CycleGAN/inx/A_%05d.png'%i).convert('RGB')\n",
    "    arr = np.array(im)\n",
    "    im = Image.fromarray(arr[40:40+240//2*3,310:70:-1]).resize((128,64*3))\n",
    "    img = np.float32(im)/255*2-1\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    r = cycleA_generate(img[None])[0]\n",
    "    im.save('CycleGAN/in/A_%05d.png'%i)\n",
    "    saveX('CycleGAN/out/A_%05d.png'%i, r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7202): #for i in range(19000,31000):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    im = Image.open('CycleGAN/inx/B_%05d.png'%i).convert('RGB')\n",
    "    arr = np.array(im)\n",
    "    im = Image.fromarray(arr[160:160+360//2*3]).resize((128,128//2*3))\n",
    "    img = np.float32(im)/255*2-1\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    r = cycleB_generate(img[None])[0]\n",
    "    im.save('CycleGAN/in/B_%05d.png'%i)\n",
    "    saveX('CycleGAN/out/B_%05d.png'%i, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(17100,32100): #for i in range(19000,31000):\n",
    "    im = Image.open('CycleGAN/A/A_%05d.png'%i).convert('RGB')\n",
    "    arr = np.array(im)\n",
    "    im = Image.fromarray(arr[40:40+240//2*3,310:70:-1]).resize((128,64*3))\n",
    "    im.save('CycleGAN/in/A_%05d.png'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(16000,31000): #for i in range(19000,31000):\n",
    "    im = Image.open('CycleGAN/B/B_%05d.png'%i).convert('RGB')\n",
    "    arr = np.array(im)\n",
    "    im = Image.fromarray(arr[160:160+360//2*3]).resize((128,128//2*3))\n",
    "    im.save('CycleGAN/in/B_%05d.png'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('model_cycle_fber.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values([netDA, netDB, netGA, netGB], param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
