{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7338, -0.1399, -0.7515]) tensor([ 11.7402,  -2.2390, -12.0240])\n",
      "tensor([ 1.0000e-01,  1.0000e+00,  1.0000e-04])\n",
      "tensor([  1.6000,  16.0000,   0.0016])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 16\n",
    "# for i in range(3):\n",
    "# while y.data.norm() < 1000:\n",
    "#     y = y * 2\n",
    "\n",
    "print x,y\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print gradients\n",
    "print x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training/testing data\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from os.path import join\n",
    "mnist_path = join('./','data','mnist')\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (fc): Linear(in_features=2304, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# construct neuron network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        ker_size = 3\n",
    "        padding = ker_size-1\n",
    "\n",
    "\n",
    "#         self.conv_lays = []\n",
    "#         in_ch = 1\n",
    "#         for i in range(3):\n",
    "#             out_ch = in_ch *2\n",
    "#             conv_lay = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "#             in_ch = out_ch\n",
    "            \n",
    "        \n",
    "        out_ch = 16\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "        \n",
    "        out_ch = out_ch*2\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_ch/2, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "        \n",
    "        out_ch = out_ch*2\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_ch/2, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "            \n",
    "        \n",
    "        self.fc = nn.Linear(out_ch*6*6, 10)\n",
    "#         Fully convolution layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "#         for conv_lay in self.conv_lays:\n",
    "#             x = conv_lay(x)\n",
    "#             x = F.relu(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        rest_dim = self.num_flat_features(x)\n",
    "        x = x.view(-1, rest_dim)\n",
    "        x = self.fc(x)\n",
    "#         x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for dim in size:\n",
    "            num_features *= dim\n",
    "        return num_features\n",
    "net = Net()\n",
    "print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,     2] loss: 0.001\n",
      "[1,     3] loss: 0.001\n",
      "[1,     4] loss: 0.001\n",
      "[1,     5] loss: 0.000\n",
      "[1,     6] loss: 0.000\n",
      "[1,     7] loss: 0.000\n",
      "[1,     8] loss: 0.001\n",
      "[1,     9] loss: 0.001\n",
      "[1,    10] loss: 0.000\n",
      "[1,    11] loss: 0.000\n",
      "[1,    12] loss: 0.000\n",
      "[1,    13] loss: 0.000\n",
      "[1,    14] loss: 0.000\n",
      "[1,    15] loss: 0.000\n",
      "[1,    16] loss: 0.000\n",
      "[1,    17] loss: 0.000\n",
      "[1,    18] loss: 0.000\n",
      "[1,    19] loss: 0.000\n",
      "[1,    20] loss: 0.000\n",
      "[1,    21] loss: 0.000\n",
      "[1,    22] loss: 0.000\n",
      "[1,    23] loss: 0.000\n",
      "[1,    24] loss: 0.000\n",
      "[1,    25] loss: 0.000\n",
      "[1,    26] loss: 0.000\n",
      "[1,    27] loss: 0.000\n",
      "[1,    28] loss: 0.000\n",
      "[1,    29] loss: 0.000\n",
      "[1,    30] loss: 0.000\n",
      "[1,    31] loss: 0.000\n",
      "[1,    32] loss: 0.000\n",
      "[1,    33] loss: 0.000\n",
      "[1,    34] loss: 0.000\n",
      "[1,    35] loss: 0.000\n",
      "[1,    36] loss: 0.000\n",
      "[1,    37] loss: 0.000\n",
      "[1,    38] loss: 0.000\n",
      "[1,    39] loss: 0.000\n",
      "[1,    40] loss: 0.000\n",
      "[1,    41] loss: 0.000\n",
      "[1,    42] loss: 0.000\n",
      "[1,    43] loss: 0.000\n",
      "[1,    44] loss: 0.000\n",
      "[1,    45] loss: 0.000\n",
      "[1,    46] loss: 0.000\n",
      "[1,    47] loss: 0.000\n",
      "[1,    48] loss: 0.000\n",
      "[1,    49] loss: 0.000\n",
      "[1,    50] loss: 0.000\n",
      "[1,    51] loss: 0.000\n",
      "[1,    52] loss: 0.000\n",
      "[1,    53] loss: 0.000\n",
      "[1,    54] loss: 0.000\n",
      "[1,    55] loss: 0.000\n",
      "[1,    56] loss: 0.000\n",
      "[1,    57] loss: 0.000\n",
      "[1,    58] loss: 0.000\n",
      "[1,    59] loss: 0.000\n",
      "[1,    60] loss: 0.000\n",
      "[1,    61] loss: 0.000\n",
      "[1,    62] loss: 0.000\n",
      "[1,    63] loss: 0.000\n",
      "[1,    64] loss: 0.000\n",
      "[1,    65] loss: 0.000\n",
      "[1,    66] loss: 0.000\n",
      "[1,    67] loss: 0.000\n",
      "[1,    68] loss: 0.000\n",
      "[1,    69] loss: 0.000\n",
      "[1,    70] loss: 0.000\n",
      "[1,    71] loss: 0.000\n",
      "[1,    72] loss: 0.000\n",
      "[1,    73] loss: 0.000\n",
      "[1,    74] loss: 0.000\n",
      "[1,    75] loss: 0.000\n",
      "[1,    76] loss: 0.000\n",
      "[1,    77] loss: 0.000\n",
      "[1,    78] loss: 0.000\n",
      "[1,    79] loss: 0.000\n",
      "[1,    80] loss: 0.000\n",
      "[1,    81] loss: 0.000\n",
      "[1,    82] loss: 0.000\n",
      "[1,    83] loss: 0.000\n",
      "[1,    84] loss: 0.000\n",
      "[1,    85] loss: 0.000\n",
      "[1,    86] loss: 0.000\n",
      "[1,    87] loss: 0.000\n",
      "[1,    88] loss: 0.000\n",
      "[1,    89] loss: 0.000\n",
      "[1,    90] loss: 0.000\n",
      "[1,    91] loss: 0.000\n",
      "[1,    92] loss: 0.000\n",
      "[1,    93] loss: 0.000\n",
      "[1,    94] loss: 0.000\n",
      "[1,    95] loss: 0.000\n",
      "[1,    96] loss: 0.000\n",
      "[1,    97] loss: 0.000\n",
      "[1,    98] loss: 0.000\n",
      "[1,    99] loss: 0.000\n",
      "[1,   100] loss: 0.000\n",
      "[1,   101] loss: 0.000\n",
      "[1,   102] loss: 0.000\n",
      "[1,   103] loss: 0.000\n",
      "[1,   104] loss: 0.000\n",
      "[1,   105] loss: 0.000\n",
      "[1,   106] loss: 0.000\n",
      "[1,   107] loss: 0.000\n",
      "[1,   108] loss: 0.000\n",
      "[1,   109] loss: 0.000\n",
      "[1,   110] loss: 0.000\n",
      "[1,   111] loss: 0.000\n",
      "[1,   112] loss: 0.000\n",
      "[1,   113] loss: 0.000\n",
      "[1,   114] loss: 0.000\n",
      "[1,   115] loss: 0.000\n",
      "[1,   116] loss: 0.000\n",
      "[1,   117] loss: 0.000\n",
      "[1,   118] loss: 0.000\n",
      "[1,   119] loss: 0.000\n",
      "[1,   120] loss: 0.000\n",
      "[1,   121] loss: 0.000\n",
      "[1,   122] loss: 0.000\n",
      "[1,   123] loss: 0.000\n",
      "[1,   124] loss: 0.000\n",
      "[1,   125] loss: 0.000\n",
      "[1,   126] loss: 0.000\n",
      "[1,   127] loss: 0.000\n",
      "[1,   128] loss: 0.000\n",
      "[1,   129] loss: 0.000\n",
      "[1,   130] loss: 0.000\n",
      "[1,   131] loss: 0.000\n",
      "[1,   132] loss: 0.000\n",
      "[1,   133] loss: 0.000\n",
      "[1,   134] loss: 0.000\n",
      "[1,   135] loss: 0.000\n",
      "[1,   136] loss: 0.000\n",
      "[1,   137] loss: 0.000\n",
      "[1,   138] loss: 0.000\n",
      "[1,   139] loss: 0.000\n",
      "[1,   140] loss: 0.000\n",
      "[1,   141] loss: 0.000\n",
      "[1,   142] loss: 0.000\n",
      "[1,   143] loss: 0.000\n",
      "[1,   144] loss: 0.000\n",
      "[1,   145] loss: 0.000\n",
      "[1,   146] loss: 0.000\n",
      "[1,   147] loss: 0.000\n",
      "[1,   148] loss: 0.000\n",
      "[1,   149] loss: 0.000\n",
      "[1,   150] loss: 0.000\n",
      "[1,   151] loss: 0.000\n",
      "[1,   152] loss: 0.000\n",
      "[1,   153] loss: 0.000\n",
      "[1,   154] loss: 0.000\n",
      "[1,   155] loss: 0.000\n",
      "[1,   156] loss: 0.000\n",
      "[1,   157] loss: 0.000\n",
      "[1,   158] loss: 0.000\n",
      "[1,   159] loss: 0.000\n",
      "[1,   160] loss: 0.000\n",
      "[1,   161] loss: 0.000\n",
      "[1,   162] loss: 0.000\n",
      "[1,   163] loss: 0.000\n",
      "[1,   164] loss: 0.000\n",
      "[1,   165] loss: 0.000\n",
      "[1,   166] loss: 0.000\n",
      "[1,   167] loss: 0.000\n",
      "[1,   168] loss: 0.000\n",
      "[1,   169] loss: 0.000\n",
      "[1,   170] loss: 0.000\n",
      "[1,   171] loss: 0.000\n",
      "[1,   172] loss: 0.000\n",
      "[1,   173] loss: 0.000\n",
      "[1,   174] loss: 0.000\n",
      "[1,   175] loss: 0.000\n",
      "[1,   176] loss: 0.000\n",
      "[1,   177] loss: 0.000\n",
      "[1,   178] loss: 0.000\n",
      "[1,   179] loss: 0.000\n",
      "[1,   180] loss: 0.000\n",
      "[1,   181] loss: 0.000\n",
      "[1,   182] loss: 0.000\n",
      "[1,   183] loss: 0.000\n",
      "[1,   184] loss: 0.000\n",
      "[1,   185] loss: 0.000\n",
      "[1,   186] loss: 0.000\n",
      "[1,   187] loss: 0.000\n",
      "[1,   188] loss: 0.000\n",
      "[1,   189] loss: 0.000\n",
      "[1,   190] loss: 0.000\n",
      "[1,   191] loss: 0.000\n",
      "[1,   192] loss: 0.000\n",
      "[1,   193] loss: 0.000\n",
      "[1,   194] loss: 0.000\n",
      "[1,   195] loss: 0.000\n",
      "[1,   196] loss: 0.000\n",
      "[1,   197] loss: 0.000\n",
      "[1,   198] loss: 0.000\n",
      "[1,   199] loss: 0.000\n",
      "[1,   200] loss: 0.000\n",
      "[1,   201] loss: 0.000\n",
      "[1,   202] loss: 0.000\n",
      "[1,   203] loss: 0.000\n",
      "[1,   204] loss: 0.000\n",
      "[1,   205] loss: 0.000\n",
      "[1,   206] loss: 0.000\n",
      "[1,   207] loss: 0.000\n",
      "[1,   208] loss: 0.000\n",
      "[1,   209] loss: 0.000\n",
      "[1,   210] loss: 0.000\n",
      "[1,   211] loss: 0.000\n",
      "[1,   212] loss: 0.000\n",
      "[1,   213] loss: 0.000\n",
      "[1,   214] loss: 0.000\n",
      "[1,   215] loss: 0.000\n",
      "[1,   216] loss: 0.000\n",
      "[1,   217] loss: 0.000\n",
      "[1,   218] loss: 0.000\n",
      "[1,   219] loss: 0.000\n",
      "[1,   220] loss: 0.000\n",
      "[1,   221] loss: 0.000\n",
      "[1,   222] loss: 0.000\n",
      "[1,   223] loss: 0.000\n",
      "[1,   224] loss: 0.000\n",
      "[1,   225] loss: 0.000\n",
      "[1,   226] loss: 0.000\n",
      "[1,   227] loss: 0.000\n",
      "[1,   228] loss: 0.000\n",
      "[1,   229] loss: 0.000\n",
      "[1,   230] loss: 0.000\n",
      "[1,   231] loss: 0.000\n",
      "[1,   232] loss: 0.000\n",
      "[1,   233] loss: 0.000\n",
      "[1,   234] loss: 0.000\n",
      "[1,   235] loss: 0.000\n",
      "[1,   236] loss: 0.000\n",
      "[1,   237] loss: 0.000\n",
      "[1,   238] loss: 0.000\n",
      "[1,   239] loss: 0.000\n",
      "[1,   240] loss: 0.000\n",
      "[1,   241] loss: 0.000\n",
      "[1,   242] loss: 0.000\n",
      "[1,   243] loss: 0.000\n",
      "[1,   244] loss: 0.000\n",
      "[1,   245] loss: 0.000\n",
      "[1,   246] loss: 0.000\n",
      "[1,   247] loss: 0.000\n",
      "[1,   248] loss: 0.000\n",
      "[1,   249] loss: 0.000\n",
      "[1,   250] loss: 0.000\n",
      "[1,   251] loss: 0.000\n",
      "[1,   252] loss: 0.000\n",
      "[1,   253] loss: 0.000\n",
      "[1,   254] loss: 0.000\n",
      "[1,   255] loss: 0.000\n",
      "[1,   256] loss: 0.000\n",
      "[1,   257] loss: 0.000\n",
      "[1,   258] loss: 0.000\n",
      "[1,   259] loss: 0.000\n",
      "[1,   260] loss: 0.000\n",
      "[1,   261] loss: 0.000\n",
      "[1,   262] loss: 0.000\n",
      "[1,   263] loss: 0.000\n",
      "[1,   264] loss: 0.000\n",
      "[1,   265] loss: 0.000\n",
      "[1,   266] loss: 0.000\n",
      "[1,   267] loss: 0.000\n",
      "[1,   268] loss: 0.000\n",
      "[1,   269] loss: 0.000\n",
      "[1,   270] loss: 0.000\n",
      "[1,   271] loss: 0.000\n",
      "[1,   272] loss: 0.000\n",
      "[1,   273] loss: 0.000\n",
      "[1,   274] loss: 0.000\n",
      "[1,   275] loss: 0.000\n",
      "[1,   276] loss: 0.000\n",
      "[1,   277] loss: 0.000\n",
      "[1,   278] loss: 0.000\n",
      "[1,   279] loss: 0.000\n",
      "[1,   280] loss: 0.000\n",
      "[1,   281] loss: 0.000\n",
      "[1,   282] loss: 0.000\n",
      "[1,   283] loss: 0.000\n",
      "[1,   284] loss: 0.000\n",
      "[1,   285] loss: 0.000\n",
      "[1,   286] loss: 0.000\n",
      "[1,   287] loss: 0.000\n",
      "[1,   288] loss: 0.000\n",
      "[1,   289] loss: 0.000\n",
      "[1,   290] loss: 0.000\n",
      "[1,   291] loss: 0.000\n",
      "[1,   292] loss: 0.000\n",
      "[1,   293] loss: 0.000\n",
      "[1,   294] loss: 0.000\n",
      "[1,   295] loss: 0.000\n",
      "[1,   296] loss: 0.000\n",
      "[1,   297] loss: 0.000\n",
      "[1,   298] loss: 0.000\n",
      "[1,   299] loss: 0.000\n",
      "[1,   300] loss: 0.000\n",
      "[1,   301] loss: 0.000\n",
      "[1,   302] loss: 0.000\n",
      "[1,   303] loss: 0.000\n",
      "[1,   304] loss: 0.000\n",
      "[1,   305] loss: 0.000\n",
      "[1,   306] loss: 0.000\n",
      "[1,   307] loss: 0.000\n",
      "[1,   308] loss: 0.000\n",
      "[1,   309] loss: 0.000\n",
      "[1,   310] loss: 0.000\n",
      "[1,   311] loss: 0.000\n",
      "[1,   312] loss: 0.000\n",
      "[1,   313] loss: 0.000\n",
      "[1,   314] loss: 0.000\n",
      "[1,   315] loss: 0.000\n",
      "[1,   316] loss: 0.000\n",
      "[1,   317] loss: 0.000\n",
      "[1,   318] loss: 0.000\n",
      "[1,   319] loss: 0.000\n",
      "[1,   320] loss: 0.000\n",
      "[1,   321] loss: 0.000\n",
      "[1,   322] loss: 0.000\n",
      "[1,   323] loss: 0.000\n",
      "[1,   324] loss: 0.000\n",
      "[1,   325] loss: 0.000\n",
      "[1,   326] loss: 0.000\n",
      "[1,   327] loss: 0.000\n",
      "[1,   328] loss: 0.000\n",
      "[1,   329] loss: 0.000\n",
      "[1,   330] loss: 0.000\n",
      "[1,   331] loss: 0.000\n",
      "[1,   332] loss: 0.000\n",
      "[1,   333] loss: 0.000\n",
      "[1,   334] loss: 0.000\n",
      "[1,   335] loss: 0.000\n",
      "[1,   336] loss: 0.000\n",
      "[1,   337] loss: 0.000\n",
      "[1,   338] loss: 0.000\n",
      "[1,   339] loss: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f5f99a13a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/payo_mac/env/python2.7.15/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/payo_mac/env/python2.7.15/lib/python2.7/site-packages/torchvision/datasets/mnist.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "with_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if with_gpu else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "epoch_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print inputs.shape\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "# environment setting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from os.path import join\n",
    "mnist_path = join('./','data','mnist')\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "for i,b in enumerate(a,0):\n",
    "    print i,b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
