{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7338, -0.1399, -0.7515]) tensor([ 11.7402,  -2.2390, -12.0240])\n",
      "tensor([ 1.0000e-01,  1.0000e+00,  1.0000e-04])\n",
      "tensor([  1.6000,  16.0000,   0.0016])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 16\n",
    "# for i in range(3):\n",
    "# while y.data.norm() < 1000:\n",
    "#     y = y * 2\n",
    "\n",
    "print x,y\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print gradients\n",
    "print x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training/testing data\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from os.path import join\n",
    "mnist_path = join('./','data','mnist')\n",
    "batch_size = 128\n",
    "# batch_size = 2\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_path, train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "  (conv4): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# construct neuron network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        ker_size = 3\n",
    "        padding = ker_size-1\n",
    "\n",
    "\n",
    "#         self.conv_lays = []\n",
    "#         in_ch = 1\n",
    "#         for i in range(3):\n",
    "#             out_ch = in_ch *2\n",
    "#             conv_lay = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "#             in_ch = out_ch\n",
    "            \n",
    "        \n",
    "        out_ch = 16\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "        \n",
    "        out_ch = out_ch*2\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_ch/2, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "        \n",
    "        out_ch = out_ch*2\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_ch/2, out_channels=out_ch, kernel_size=3, stride=2, padding=padding)\n",
    "        \n",
    "            \n",
    "        self.conv4 = nn.Conv2d(in_channels=out_ch, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "#         self.fc = nn.Linear(out_ch*6*6, 10)\n",
    "#         Fully convolution layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "#         for conv_lay in self.conv_lays:\n",
    "#             x = conv_lay(x)\n",
    "#             x = F.relu(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        rest_dim = self.num_flat_features(x)\n",
    "        x = x.view(-1, rest_dim)\n",
    "#         x = nn.AlphaDropout(0.5)(x)\n",
    "#         x = self.fc(x)\n",
    "#         x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for dim in size:\n",
    "            num_features *= dim\n",
    "        return num_features\n",
    "net = Net()\n",
    "print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    11] loss: 0.013 train_acc: 0.227 test_acc: 0.211\n",
      "[1,    21] loss: 0.011 train_acc: 0.289 test_acc: 0.281\n",
      "[1,    31] loss: 0.009 train_acc: 0.367 test_acc: 0.383\n",
      "[1,    41] loss: 0.009 train_acc: 0.391 test_acc: 0.398\n",
      "[1,    51] loss: 0.008 train_acc: 0.523 test_acc: 0.555\n",
      "[1,    61] loss: 0.008 train_acc: 0.617 test_acc: 0.531\n",
      "[1,    71] loss: 0.008 train_acc: 0.523 test_acc: 0.539\n",
      "[1,    81] loss: 0.007 train_acc: 0.555 test_acc: 0.453\n",
      "[1,    91] loss: 0.007 train_acc: 0.562 test_acc: 0.531\n",
      "[1,   101] loss: 0.007 train_acc: 0.531 test_acc: 0.477\n",
      "[1,   111] loss: 0.007 train_acc: 0.469 test_acc: 0.633\n",
      "[1,   121] loss: 0.007 train_acc: 0.602 test_acc: 0.594\n",
      "[1,   131] loss: 0.007 train_acc: 0.555 test_acc: 0.508\n",
      "[1,   141] loss: 0.007 train_acc: 0.609 test_acc: 0.609\n",
      "[1,   151] loss: 0.007 train_acc: 0.477 test_acc: 0.594\n",
      "[1,   161] loss: 0.007 train_acc: 0.555 test_acc: 0.609\n",
      "[1,   171] loss: 0.007 train_acc: 0.516 test_acc: 0.484\n",
      "[1,   181] loss: 0.007 train_acc: 0.539 test_acc: 0.594\n",
      "[1,   191] loss: 0.007 train_acc: 0.570 test_acc: 0.547\n",
      "[1,   201] loss: 0.007 train_acc: 0.539 test_acc: 0.484\n",
      "[1,   211] loss: 0.007 train_acc: 0.602 test_acc: 0.594\n",
      "[1,   221] loss: 0.007 train_acc: 0.562 test_acc: 0.602\n",
      "[1,   231] loss: 0.007 train_acc: 0.594 test_acc: 0.656\n",
      "[1,   241] loss: 0.007 train_acc: 0.555 test_acc: 0.586\n",
      "[1,   251] loss: 0.006 train_acc: 0.648 test_acc: 0.539\n",
      "[1,   261] loss: 0.007 train_acc: 0.555 test_acc: 0.586\n",
      "[1,   271] loss: 0.007 train_acc: 0.570 test_acc: 0.602\n",
      "[1,   281] loss: 0.007 train_acc: 0.570 test_acc: 0.641\n",
      "[1,   291] loss: 0.006 train_acc: 0.703 test_acc: 0.578\n",
      "[1,   301] loss: 0.007 train_acc: 0.555 test_acc: 0.562\n",
      "[1,   311] loss: 0.007 train_acc: 0.562 test_acc: 0.562\n",
      "[1,   321] loss: 0.007 train_acc: 0.469 test_acc: 0.508\n",
      "[1,   331] loss: 0.006 train_acc: 0.617 test_acc: 0.711\n",
      "[1,   341] loss: 0.006 train_acc: 0.648 test_acc: 0.539\n",
      "[1,   351] loss: 0.006 train_acc: 0.570 test_acc: 0.578\n",
      "[1,   361] loss: 0.006 train_acc: 0.594 test_acc: 0.617\n",
      "[1,   371] loss: 0.007 train_acc: 0.578 test_acc: 0.555\n",
      "[1,   381] loss: 0.006 train_acc: 0.625 test_acc: 0.633\n",
      "[1,   391] loss: 0.007 train_acc: 0.602 test_acc: 0.562\n",
      "[1,   401] loss: 0.007 train_acc: 0.570 test_acc: 0.633\n",
      "[1,   411] loss: 0.007 train_acc: 0.602 test_acc: 0.609\n",
      "[1,   421] loss: 0.007 train_acc: 0.555 test_acc: 0.570\n",
      "[1,   431] loss: 0.006 train_acc: 0.633 test_acc: 0.625\n",
      "[1,   441] loss: 0.007 train_acc: 0.695 test_acc: 0.555\n",
      "[1,   451] loss: 0.006 train_acc: 0.562 test_acc: 0.602\n",
      "[1,   461] loss: 0.006 train_acc: 0.648 test_acc: 0.625\n",
      "[2,    11] loss: 0.007 train_acc: 0.617 test_acc: 0.594\n",
      "[2,    21] loss: 0.007 train_acc: 0.531 test_acc: 0.609\n",
      "[2,    31] loss: 0.007 train_acc: 0.625 test_acc: 0.609\n",
      "[2,    41] loss: 0.006 train_acc: 0.719 test_acc: 0.539\n",
      "[2,    51] loss: 0.007 train_acc: 0.656 test_acc: 0.539\n",
      "[2,    61] loss: 0.006 train_acc: 0.586 test_acc: 0.625\n",
      "[2,    71] loss: 0.006 train_acc: 0.664 test_acc: 0.570\n",
      "[2,    81] loss: 0.006 train_acc: 0.625 test_acc: 0.617\n",
      "[2,    91] loss: 0.006 train_acc: 0.625 test_acc: 0.633\n",
      "[2,   101] loss: 0.006 train_acc: 0.672 test_acc: 0.609\n",
      "[2,   111] loss: 0.006 train_acc: 0.547 test_acc: 0.602\n",
      "[2,   121] loss: 0.006 train_acc: 0.617 test_acc: 0.570\n",
      "[2,   131] loss: 0.006 train_acc: 0.555 test_acc: 0.656\n",
      "[2,   141] loss: 0.006 train_acc: 0.641 test_acc: 0.539\n",
      "[2,   151] loss: 0.006 train_acc: 0.539 test_acc: 0.586\n",
      "[2,   161] loss: 0.007 train_acc: 0.562 test_acc: 0.539\n",
      "[2,   171] loss: 0.006 train_acc: 0.609 test_acc: 0.555\n",
      "[2,   181] loss: 0.006 train_acc: 0.594 test_acc: 0.578\n",
      "[2,   191] loss: 0.006 train_acc: 0.656 test_acc: 0.633\n",
      "[2,   201] loss: 0.007 train_acc: 0.688 test_acc: 0.633\n",
      "[2,   211] loss: 0.006 train_acc: 0.586 test_acc: 0.648\n",
      "[2,   221] loss: 0.006 train_acc: 0.570 test_acc: 0.531\n",
      "[2,   231] loss: 0.006 train_acc: 0.633 test_acc: 0.633\n",
      "[2,   241] loss: 0.006 train_acc: 0.633 test_acc: 0.570\n",
      "[2,   251] loss: 0.006 train_acc: 0.633 test_acc: 0.562\n",
      "[2,   261] loss: 0.007 train_acc: 0.633 test_acc: 0.641\n",
      "[2,   271] loss: 0.007 train_acc: 0.656 test_acc: 0.633\n",
      "[2,   281] loss: 0.007 train_acc: 0.578 test_acc: 0.586\n",
      "[2,   291] loss: 0.006 train_acc: 0.609 test_acc: 0.656\n",
      "[2,   301] loss: 0.007 train_acc: 0.625 test_acc: 0.625\n",
      "[2,   311] loss: 0.006 train_acc: 0.617 test_acc: 0.539\n",
      "[2,   321] loss: 0.006 train_acc: 0.539 test_acc: 0.609\n",
      "[2,   331] loss: 0.007 train_acc: 0.562 test_acc: 0.617\n",
      "[2,   341] loss: 0.006 train_acc: 0.531 test_acc: 0.656\n",
      "[2,   351] loss: 0.006 train_acc: 0.641 test_acc: 0.531\n",
      "[2,   361] loss: 0.006 train_acc: 0.680 test_acc: 0.578\n",
      "[2,   371] loss: 0.007 train_acc: 0.641 test_acc: 0.570\n",
      "[2,   381] loss: 0.007 train_acc: 0.562 test_acc: 0.602\n",
      "[2,   391] loss: 0.006 train_acc: 0.562 test_acc: 0.539\n",
      "[2,   401] loss: 0.006 train_acc: 0.602 test_acc: 0.562\n",
      "[2,   411] loss: 0.007 train_acc: 0.555 test_acc: 0.695\n",
      "[2,   421] loss: 0.007 train_acc: 0.562 test_acc: 0.672\n",
      "[2,   431] loss: 0.006 train_acc: 0.586 test_acc: 0.516\n",
      "[2,   441] loss: 0.006 train_acc: 0.625 test_acc: 0.516\n",
      "[2,   451] loss: 0.006 train_acc: 0.672 test_acc: 0.625\n",
      "[2,   461] loss: 0.006 train_acc: 0.516 test_acc: 0.664\n",
      "[3,    11] loss: 0.007 train_acc: 0.562 test_acc: 0.484\n",
      "[3,    21] loss: 0.006 train_acc: 0.648 test_acc: 0.641\n",
      "[3,    31] loss: 0.006 train_acc: 0.586 test_acc: 0.602\n",
      "[3,    41] loss: 0.006 train_acc: 0.586 test_acc: 0.602\n",
      "[3,    51] loss: 0.007 train_acc: 0.625 test_acc: 0.672\n",
      "[3,    61] loss: 0.006 train_acc: 0.672 test_acc: 0.547\n",
      "[3,    71] loss: 0.006 train_acc: 0.625 test_acc: 0.641\n",
      "[3,    81] loss: 0.006 train_acc: 0.633 test_acc: 0.539\n",
      "[3,    91] loss: 0.006 train_acc: 0.570 test_acc: 0.539\n",
      "[3,   101] loss: 0.007 train_acc: 0.602 test_acc: 0.609\n",
      "[3,   111] loss: 0.006 train_acc: 0.594 test_acc: 0.578\n",
      "[3,   121] loss: 0.006 train_acc: 0.508 test_acc: 0.633\n",
      "[3,   131] loss: 0.006 train_acc: 0.594 test_acc: 0.633\n",
      "[3,   141] loss: 0.006 train_acc: 0.633 test_acc: 0.555\n",
      "[3,   151] loss: 0.006 train_acc: 0.641 test_acc: 0.578\n",
      "[3,   161] loss: 0.007 train_acc: 0.672 test_acc: 0.625\n",
      "[3,   171] loss: 0.007 train_acc: 0.602 test_acc: 0.570\n",
      "[3,   181] loss: 0.006 train_acc: 0.602 test_acc: 0.531\n",
      "[3,   191] loss: 0.006 train_acc: 0.664 test_acc: 0.594\n",
      "[3,   201] loss: 0.006 train_acc: 0.578 test_acc: 0.531\n",
      "[3,   211] loss: 0.007 train_acc: 0.703 test_acc: 0.523\n",
      "[3,   221] loss: 0.006 train_acc: 0.633 test_acc: 0.562\n",
      "[3,   231] loss: 0.006 train_acc: 0.633 test_acc: 0.680\n",
      "[3,   241] loss: 0.006 train_acc: 0.758 test_acc: 0.703\n",
      "[3,   251] loss: 0.006 train_acc: 0.586 test_acc: 0.633\n",
      "[3,   261] loss: 0.006 train_acc: 0.680 test_acc: 0.625\n",
      "[3,   271] loss: 0.006 train_acc: 0.664 test_acc: 0.617\n",
      "[3,   281] loss: 0.006 train_acc: 0.531 test_acc: 0.609\n",
      "[3,   291] loss: 0.006 train_acc: 0.711 test_acc: 0.617\n",
      "[3,   301] loss: 0.006 train_acc: 0.562 test_acc: 0.602\n",
      "[3,   311] loss: 0.006 train_acc: 0.562 test_acc: 0.625\n",
      "[3,   321] loss: 0.006 train_acc: 0.602 test_acc: 0.586\n",
      "[3,   331] loss: 0.006 train_acc: 0.578 test_acc: 0.586\n",
      "[3,   341] loss: 0.006 train_acc: 0.641 test_acc: 0.633\n",
      "[3,   351] loss: 0.006 train_acc: 0.680 test_acc: 0.555\n",
      "[3,   361] loss: 0.007 train_acc: 0.570 test_acc: 0.688\n",
      "[3,   371] loss: 0.006 train_acc: 0.555 test_acc: 0.602\n",
      "[3,   381] loss: 0.006 train_acc: 0.555 test_acc: 0.672\n",
      "[3,   391] loss: 0.007 train_acc: 0.492 test_acc: 0.602\n",
      "[3,   401] loss: 0.006 train_acc: 0.562 test_acc: 0.617\n",
      "[3,   411] loss: 0.006 train_acc: 0.562 test_acc: 0.633\n",
      "[3,   421] loss: 0.006 train_acc: 0.570 test_acc: 0.562\n",
      "[3,   431] loss: 0.006 train_acc: 0.633 test_acc: 0.508\n",
      "[3,   441] loss: 0.007 train_acc: 0.523 test_acc: 0.570\n",
      "[3,   451] loss: 0.006 train_acc: 0.641 test_acc: 0.664\n",
      "[3,   461] loss: 0.006 train_acc: 0.617 test_acc: 0.641\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0cf1878897a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torchvision/datasets/mnist.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2448\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2378\u001b[0m     \"\"\"\n\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2380\u001b[0;31m     \u001b[0m_check_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m     \u001b[0;31m# may pass tuple instead of argument list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "\n",
    "with_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if with_gpu else \"cpu\")\n",
    "model = net.to(device)\n",
    "\n",
    "def acc(net, data):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(inputs)\n",
    "        max_vals, max_indices = torch.max(outputs,1)\n",
    "        acc = (max_indices == labels).sum().cpu().data.numpy()/ float(max_indices.size()[0])\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "epoch_num = 100\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    i = 1\n",
    "    for data, test_data in zip(train_loader, train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print inputs.shape\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc = acc(net, data)\n",
    "        test_acc = acc(net, test_data)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f train_acc: %.3f test_acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000, train_acc, test_acc))\n",
    "            running_loss = 0.0\n",
    "        i+=1\n",
    "\n",
    "\n",
    "\n",
    "# environment setting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
